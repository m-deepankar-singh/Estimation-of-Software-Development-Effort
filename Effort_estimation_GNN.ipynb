{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66adaTh7k6JY"
      },
      "source": [
        "# **Graph Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qbw_jSzcR4Fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in c:\\users\\singh\\anaconda3\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\singh\\appdata\\roaming\\python\\python310\\site-packages (from torch_geometric) (5.9.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (1.2.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (4.64.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (1.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\singh\\anaconda3\\lib\\site-packages (from torch_geometric) (2.28.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\singh\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\singh\\anaconda3\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dynvdJCDd14J"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kR48619u51Zw"
      },
      "outputs": [],
      "source": [
        "# Load the ISBSG dataset\n",
        "df = pd.read_csv(\"ISBSG.csv\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "# Dropping the first column 'Project' as it is not needed for the analysis\n",
        "df = df.drop('Project ID', axis=1)\n",
        "df = df.drop('Data Quality Rating', axis=1)\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "features = df.drop('Summary Work Effort', axis=1)\n",
        "labels = df['Summary Work Effort']\n",
        "\n",
        "# Use PCA to reduce the dimensionality of the features\n",
        "pca = PCA(n_components=10) # set the number of components to keep\n",
        "features = pca.fit_transform(features)\n",
        "\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "# Converting the dataset to a NetworkX graph\n",
        "# Here, we create a fully connected graph where edge weights are the cosine similarity between feature vectors\n",
        "\n",
        "\n",
        "G = nx.Graph()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###**DO NOT RUN THIS CELL** \n",
        "if you have already converted the dataset to a graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(features)):\n",
        "    for j in range(i+1, len(features)):\n",
        "        similarity = cosine_similarity(features[i].reshape(1, -1), features[j].reshape(1, -1))\n",
        "        G.add_edge(i, j, weight=similarity[0][0])\n",
        "\n",
        "# Save the graph to a file\n",
        "with open(\"graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(G, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lbbAOQhok--x"
      },
      "outputs": [],
      "source": [
        "# Save the graph to a file\n",
        "with open(\"C:\\Work\\Estimation-of-Software-Development-Effort-main\\graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(G, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "81fHKDGRWLIX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPU\n",
            "Epoch: 1, Loss: 505026976.0000\n",
            "Epoch: 2, Loss: 505027168.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\singh\\anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Loss: 505026432.0000\n",
            "Epoch: 4, Loss: 505022944.0000\n",
            "Epoch: 5, Loss: 505024032.0000\n",
            "Epoch: 6, Loss: 505017056.0000\n",
            "Epoch: 7, Loss: 505022720.0000\n",
            "Epoch: 8, Loss: 505016576.0000\n",
            "Epoch: 9, Loss: 505017536.0000\n",
            "Epoch: 10, Loss: 505016256.0000\n",
            "Epoch: 11, Loss: 505012096.0000\n",
            "Epoch: 12, Loss: 505008000.0000\n",
            "Epoch: 13, Loss: 505003744.0000\n",
            "Epoch: 14, Loss: 504990752.0000\n",
            "Epoch: 15, Loss: 504988224.0000\n",
            "Epoch: 16, Loss: 504991168.0000\n",
            "Epoch: 17, Loss: 504988320.0000\n",
            "Epoch: 18, Loss: 504988416.0000\n",
            "Epoch: 19, Loss: 504947328.0000\n",
            "Epoch: 20, Loss: 504977120.0000\n",
            "Epoch: 21, Loss: 504974016.0000\n",
            "Epoch: 22, Loss: 504965344.0000\n",
            "Epoch: 23, Loss: 504904544.0000\n",
            "Epoch: 24, Loss: 504920352.0000\n",
            "Epoch: 25, Loss: 504918112.0000\n",
            "Epoch: 26, Loss: 504887264.0000\n",
            "Epoch: 27, Loss: 504886720.0000\n",
            "Epoch: 28, Loss: 504927776.0000\n",
            "Epoch: 29, Loss: 504849280.0000\n",
            "Epoch: 30, Loss: 504893024.0000\n",
            "Epoch: 31, Loss: 504732512.0000\n",
            "Epoch: 32, Loss: 504843104.0000\n",
            "Epoch: 33, Loss: 504786112.0000\n",
            "Epoch: 34, Loss: 504765312.0000\n",
            "Epoch: 35, Loss: 504827488.0000\n",
            "Epoch: 36, Loss: 504789056.0000\n",
            "Epoch: 37, Loss: 504815296.0000\n",
            "Epoch: 38, Loss: 504819456.0000\n",
            "Epoch: 39, Loss: 504713536.0000\n",
            "Epoch: 40, Loss: 504702720.0000\n",
            "Epoch: 41, Loss: 504819200.0000\n",
            "Epoch: 42, Loss: 504766912.0000\n",
            "Epoch: 43, Loss: 504773024.0000\n",
            "Epoch: 44, Loss: 504771808.0000\n",
            "Epoch: 45, Loss: 504828608.0000\n",
            "Epoch: 46, Loss: 504669984.0000\n",
            "Epoch: 47, Loss: 504858400.0000\n",
            "Epoch: 48, Loss: 504786464.0000\n",
            "Epoch: 49, Loss: 504792096.0000\n",
            "Epoch: 50, Loss: 504788576.0000\n",
            "Epoch: 51, Loss: 504714240.0000\n",
            "Epoch: 52, Loss: 504832736.0000\n",
            "Epoch: 53, Loss: 504879104.0000\n",
            "Epoch: 54, Loss: 504777888.0000\n",
            "Epoch: 55, Loss: 504748960.0000\n",
            "Epoch: 56, Loss: 504749248.0000\n",
            "Epoch: 57, Loss: 504664256.0000\n",
            "Epoch: 58, Loss: 504791296.0000\n",
            "Epoch: 59, Loss: 504737280.0000\n",
            "Epoch: 60, Loss: 504705632.0000\n",
            "Epoch: 61, Loss: 504675520.0000\n",
            "Epoch: 62, Loss: 504792160.0000\n",
            "Epoch: 63, Loss: 504740672.0000\n",
            "Epoch: 64, Loss: 504744736.0000\n",
            "Epoch: 65, Loss: 504780192.0000\n",
            "Epoch: 66, Loss: 504844288.0000\n",
            "Epoch: 67, Loss: 504812992.0000\n",
            "Epoch: 68, Loss: 504839712.0000\n",
            "Epoch: 69, Loss: 504820096.0000\n",
            "Epoch: 70, Loss: 504759008.0000\n",
            "Epoch: 71, Loss: 504831808.0000\n",
            "Epoch: 72, Loss: 504778944.0000\n",
            "Epoch: 73, Loss: 504743872.0000\n",
            "Epoch: 74, Loss: 504886560.0000\n",
            "Epoch: 75, Loss: 504774336.0000\n",
            "Epoch: 76, Loss: 504778752.0000\n",
            "Epoch: 77, Loss: 504676832.0000\n",
            "Epoch: 78, Loss: 504810752.0000\n",
            "Epoch: 79, Loss: 504801504.0000\n",
            "Epoch: 80, Loss: 504876000.0000\n",
            "Epoch: 81, Loss: 504696640.0000\n",
            "Epoch: 82, Loss: 504782912.0000\n",
            "Epoch: 83, Loss: 504802112.0000\n",
            "Epoch: 84, Loss: 504772096.0000\n",
            "Epoch: 85, Loss: 504791840.0000\n",
            "Epoch: 86, Loss: 504768192.0000\n",
            "Epoch: 87, Loss: 504751040.0000\n",
            "Epoch: 88, Loss: 504690336.0000\n",
            "Epoch: 89, Loss: 504730720.0000\n",
            "Epoch: 90, Loss: 504788384.0000\n",
            "Epoch: 91, Loss: 504734528.0000\n",
            "Epoch: 92, Loss: 504767584.0000\n",
            "Epoch: 93, Loss: 504756320.0000\n",
            "Epoch: 94, Loss: 504787360.0000\n",
            "Epoch: 95, Loss: 504809696.0000\n",
            "Epoch: 96, Loss: 504695456.0000\n",
            "Epoch: 97, Loss: 504766944.0000\n",
            "Epoch: 98, Loss: 504672192.0000\n",
            "Epoch: 99, Loss: 504756352.0000\n",
            "Epoch: 100, Loss: 504768288.0000\n",
            "Epoch: 101, Loss: 504753376.0000\n",
            "Epoch: 102, Loss: 504811936.0000\n",
            "Epoch: 103, Loss: 504788224.0000\n",
            "Epoch: 104, Loss: 504853120.0000\n",
            "Epoch: 105, Loss: 504784448.0000\n",
            "Epoch: 106, Loss: 504739264.0000\n",
            "Epoch: 107, Loss: 504819392.0000\n",
            "Epoch: 108, Loss: 504713088.0000\n",
            "Epoch: 109, Loss: 504740032.0000\n",
            "Epoch: 110, Loss: 504737472.0000\n",
            "Epoch: 111, Loss: 504652256.0000\n",
            "Epoch: 112, Loss: 504791744.0000\n",
            "Epoch: 113, Loss: 504790336.0000\n",
            "Epoch: 114, Loss: 504838080.0000\n",
            "Epoch: 115, Loss: 504677952.0000\n",
            "Epoch: 116, Loss: 504856096.0000\n",
            "Epoch: 117, Loss: 504667072.0000\n",
            "Epoch: 118, Loss: 504773280.0000\n",
            "Epoch: 119, Loss: 504817600.0000\n",
            "Epoch: 120, Loss: 504772480.0000\n",
            "Epoch: 121, Loss: 504704256.0000\n",
            "Epoch: 122, Loss: 504783200.0000\n",
            "Epoch: 123, Loss: 504764448.0000\n",
            "Epoch: 124, Loss: 504708608.0000\n",
            "Epoch: 125, Loss: 504684352.0000\n",
            "Epoch: 126, Loss: 504802208.0000\n",
            "Epoch: 127, Loss: 504763136.0000\n",
            "Epoch: 128, Loss: 504678240.0000\n",
            "Epoch: 129, Loss: 504691136.0000\n",
            "Epoch: 130, Loss: 504753056.0000\n",
            "Epoch: 131, Loss: 504850784.0000\n",
            "Epoch: 132, Loss: 504778080.0000\n",
            "Epoch: 133, Loss: 504716832.0000\n",
            "Epoch: 134, Loss: 504784960.0000\n",
            "Epoch: 135, Loss: 504838976.0000\n",
            "Epoch: 136, Loss: 504839872.0000\n",
            "Epoch: 137, Loss: 504740896.0000\n",
            "Epoch: 138, Loss: 504691488.0000\n",
            "Epoch: 139, Loss: 504680544.0000\n",
            "Epoch: 140, Loss: 504745568.0000\n",
            "Epoch: 141, Loss: 504808032.0000\n",
            "Epoch: 142, Loss: 504710112.0000\n",
            "Epoch: 143, Loss: 504760416.0000\n",
            "Epoch: 144, Loss: 504787424.0000\n",
            "Epoch: 145, Loss: 504751104.0000\n",
            "Epoch: 146, Loss: 504654848.0000\n",
            "Epoch: 147, Loss: 504829664.0000\n",
            "Epoch: 148, Loss: 504767680.0000\n",
            "Epoch: 149, Loss: 504569504.0000\n",
            "Epoch: 150, Loss: 504779520.0000\n",
            "Epoch: 151, Loss: 504617184.0000\n",
            "Epoch: 152, Loss: 504682656.0000\n",
            "Epoch: 153, Loss: 504922624.0000\n",
            "Epoch: 154, Loss: 504746112.0000\n",
            "Epoch: 155, Loss: 504838304.0000\n",
            "Epoch: 156, Loss: 504757760.0000\n",
            "Epoch: 157, Loss: 504823936.0000\n",
            "Epoch: 158, Loss: 504772096.0000\n",
            "Epoch: 159, Loss: 504757920.0000\n",
            "Epoch: 160, Loss: 504725824.0000\n",
            "Epoch: 161, Loss: 504766368.0000\n",
            "Epoch: 162, Loss: 504809984.0000\n",
            "Epoch: 163, Loss: 504746784.0000\n",
            "Epoch: 164, Loss: 504777696.0000\n",
            "Epoch: 165, Loss: 504842144.0000\n",
            "Epoch: 166, Loss: 504727904.0000\n",
            "Epoch: 167, Loss: 504680992.0000\n",
            "Epoch: 168, Loss: 504846368.0000\n",
            "Epoch: 169, Loss: 504778080.0000\n",
            "Epoch: 170, Loss: 504825984.0000\n",
            "Epoch: 171, Loss: 504721504.0000\n",
            "Epoch: 172, Loss: 504843008.0000\n",
            "Epoch: 173, Loss: 504844000.0000\n",
            "Epoch: 174, Loss: 504769536.0000\n",
            "Epoch: 175, Loss: 504778368.0000\n",
            "Epoch: 176, Loss: 504746176.0000\n",
            "Epoch: 177, Loss: 504817600.0000\n",
            "Epoch: 178, Loss: 504786880.0000\n",
            "Epoch: 179, Loss: 504832864.0000\n",
            "Epoch: 180, Loss: 504820736.0000\n",
            "Epoch: 181, Loss: 504740448.0000\n",
            "Epoch: 182, Loss: 504708800.0000\n",
            "Epoch: 183, Loss: 504810336.0000\n",
            "Epoch: 184, Loss: 504727264.0000\n",
            "Epoch: 185, Loss: 504836480.0000\n",
            "Epoch: 186, Loss: 504761184.0000\n",
            "Epoch: 187, Loss: 504811040.0000\n",
            "Epoch: 188, Loss: 504735296.0000\n",
            "Epoch: 189, Loss: 504838464.0000\n",
            "Epoch: 190, Loss: 504896032.0000\n",
            "Epoch: 191, Loss: 504723232.0000\n",
            "Epoch: 192, Loss: 504719136.0000\n",
            "Epoch: 193, Loss: 504775040.0000\n",
            "Epoch: 194, Loss: 504780544.0000\n",
            "Epoch: 195, Loss: 504723936.0000\n",
            "Epoch: 196, Loss: 504705952.0000\n",
            "Epoch: 197, Loss: 504825984.0000\n",
            "Epoch: 198, Loss: 504815520.0000\n",
            "Epoch: 199, Loss: 504707424.0000\n",
            "Epoch: 200, Loss: 504709824.0000\n"
          ]
        }
      ],
      "source": [
        "# Convert NetworkX graph to PyTorch Geometric Data\n",
        "edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()\n",
        "x = torch.tensor(features, dtype=torch.float)\n",
        "y = torch.tensor(labels.values, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Define the GNN architecture\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_features, 64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.conv3 = GCNConv(64, 32)\n",
        "        self.conv4 = GCNConv(32, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv4(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "\n",
        "# Create the model, optimizer, and data loader\n",
        "model = GNN().to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.004)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.001)\n",
        "loader = DataLoader([data], batch_size=32, shuffle=True)\n",
        "\n",
        "# Train the GNN model\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Train the model for multiple epochs\n",
        "for epoch in range(200):\n",
        "    loss = train()\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgQYtXACjDDm"
      },
      "source": [
        "In the above code, `loss` is the objective that the GNN model is trying to minimize during training. Specifically, it's the Mean Squared Error (MSE) loss between the predicted effort values (`out`) and the actual effort values (`data.y`). The goal of the training process is to find the model parameters that minimize the MSE loss.\n",
        "\n",
        "Here's a brief breakdown of the key lines related to the loss:\n",
        "\n",
        "```python\n",
        "out = model(data)  # Get predicted effort values from the GNN model\n",
        "loss = F.mse_loss(out, data.y)  # Compute the MSE loss between predictions and actual effort values\n",
        "loss.backward()  # Compute the gradients of the loss with respect to the model parameters\n",
        "optimizer.step()  # Update the model parameters using the computed gradients\n",
        "```\n",
        "\n",
        "During each training epoch, the model processes the data, makes predictions, and updates its parameters based on the gradients of the loss function to reduce the error in the predictions. The `loss` variable is used to keep track of how well the model is performing during training and to provide feedback on the learning progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoMFZDlwnKTK",
        "outputId": "57da739f-fc93-4af3-849f-f19aa429bcfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error (MAE): 6043.5483\n",
            "Root Mean Squared Error (RMSE): 22466.8828\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move the input data to the same device as the model\n",
        "data = data.to(device)\n",
        "\n",
        "# Get the predicted effort values\n",
        "with torch.no_grad():\n",
        "    predicted_effort = model(data)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Calculate the MAE and RMSE\n",
        "mae = mean_absolute_error(data.y.cpu().numpy(), predicted_effort.cpu().numpy())\n",
        "rmse = np.sqrt(mean_squared_error(data.y.cpu().numpy(), predicted_effort.cpu().numpy()))\n",
        "\n",
        "# Print the MAE and RMSE\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WomkU0Hapd4o",
        "outputId": "b0119af0-21af-4bf2-db9d-20c011167e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Magnitude of Relative Error (MMRE): 0.9713\n"
          ]
        }
      ],
      "source": [
        "def mean_magnitude_relative_error(y_true, y_pred):\n",
        "    relative_errors = np.abs(y_true - y_pred) / y_true\n",
        "    return np.mean(relative_errors)\n",
        "\n",
        "# Calculate the MMRE\n",
        "mmre = mean_magnitude_relative_error(data.y.cpu().numpy(), predicted_effort.cpu().numpy())\n",
        "\n",
        "# Print the MMRE\n",
        "print(f\"Mean Magnitude of Relative Error (MMRE): {mmre:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym59hosAu55g"
      },
      "source": [
        "To improve the model's performance, you can try:\n",
        "\n",
        "1. Increase the number of training epochs or adjust the learning rate.\n",
        "2. Experiment with different GNN architectures or other machine learning models.\n",
        "3. Fine-tune the process of converting the dataset into a graph representation.\n",
        "4. Perform feature engineering, feature selection, or dimensionality reduction to improve the quality of the input features.\n",
        "5. Divide your dataset into training and testing sets to better evaluate the model's performance and avoid overfitting."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
